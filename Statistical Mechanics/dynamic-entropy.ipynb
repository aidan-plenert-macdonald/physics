{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic computation of entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy model\n",
    "\n",
    "Consider the computation of entropy for a Bernoulli random variable $ X \\sim \\mathcal{B}(\\rho) $. If we consider a sample estimate from sample size $ N $ with $ n = \\rho N $, then the entropy can be estimated via,\n",
    "\n",
    "$$\\begin{align}\n",
    "H &= - \\frac{n}{N} \\log \\frac{n}{N} - \\frac{N - n}{N} \\log \\frac{N - n}{N} \\\\\n",
    "  &= - \\frac{n \\log n + (n - N) \\log (N - n)}{N} + \\log N \\\\\n",
    "  &= - \\frac{n}{N} \\log n(N - n) + \\log N(N - n)\n",
    "\\end{align}$$\n",
    "\n",
    "If we are estimating this via sampling, then consider the difference in entropy between samples $ H(n + \\Delta n, N+1) - H(n, N) $,\n",
    "\n",
    "First, define,\n",
    "$$\\begin{align}\n",
    "a(n, N) &= n(N-n) \\\\\n",
    "A(n, N) &= N(N-n) \\\\\n",
    "H &= - \\frac{a}{A} \\log a + \\log A\n",
    "\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Delta H &= - \\frac{a'}{A'} \\log a' + \\log A' + \\frac{a}{A} \\log a - \\log A \\\\\n",
    "  &= \n",
    "\\end{align}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
